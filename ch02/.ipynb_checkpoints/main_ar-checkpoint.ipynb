{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914f14b64115aded",
   "metadata": {},
   "source": [
    "# التعامل مع النصوص #\n",
    "\n",
    "سوف نستعرض هنا كيفية تحضير النصوص العربية لاستخدامها في تدريب نماذج اللغة الكبيرة.\n",
    "\n",
    "سوف نقوم في البداية بتحويل النص إلى ترميزات، حيث تمثل كل ترميزة كلمة أو جزء من كلمة. بعد ذلك نقوم بتشفير الترميزات باستخدام متجهات متعددة الأبعاد تسمى تضمينات، يمكن استخدامها لتدريب نماذج اللغة الكبيرة.\n",
    "\n",
    "تجدر الأشارة إلى أن النص العربي يختلف عن غيره من اللغات مثل الإنجليزية بأنه يكتب من اليمين إلى اليسار و باحتوائه على الحركات أو التشكيل، مما يتطلب التعامل معه بشكل مختلف في بعض الأمور."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f31a3d1879071",
   "metadata": {},
   "source": [
    " ## تضمينات الكلمات\n",
    "يتعذر على نماذج التعليم العميق، بما في ذلك نماذج اللغة الكبيرة، من معالجة النصوص بشكل مباشر. لذلك يجب علينا تمثيل الكلمات كمتجهات متعددة الأبعاد تسمى تضمينات، ويُشار إلى هذه العملية باسم تضمين الكلمات."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650dbd08cb40ce",
   "metadata": {},
   "source": [
    "\n",
    " ## ترميز النص\n",
    "للبدء بتضمين الكلمات، يجب علينا أولا تقسيم النص إلى أجزاء تسمى ترميزات.\n",
    "\n",
    "لنرى كيف يمكن تجزئة جملة قصيرة باستخدام الدالية  `re.split` مع الحفاظ على الفواصل:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365e647f4bd7231b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:15.545662Z",
     "start_time": "2025-12-26T20:32:15.534024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مرحبا', '،', 'بالعالم', '.', 'هل', 'هذا', '—', 'اختبار', '؟']\n",
      "مرحبا\n",
      "؟\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "RE_ENCODER = r'([،.؟!:؛«»—]|\\s)'\n",
    "RE_DECODER = r'\\s+([،.؟!:؛—])'\n",
    "\n",
    "text = \"مرحبا، بالعالم. هل هذا— اختبار؟\"\n",
    "result = re.split(RE_ENCODER, text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n",
    "print(result[0])\n",
    "print(result[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab64420a72dca5",
   "metadata": {},
   "source": [
    "Notice that “right-to-left” display of Arabic text is a rendering/visual property, not how the string is stored or indexed.\n",
    "\n",
    "In Python, a `str` is a sequence of Unicode code points stored in the order it was typed in (the logical order), so `result[0] = مرحبا` and `result[-1] = ؟` regardless of whether it's displayed right-to-left or left-to-right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1abb6603663640",
   "metadata": {},
   "source": [
    "The text we will tokenize for LLM training is \"مغامرة العميل المرموق\", which has been released into the public domain and is thus permitted to be used for LLM training tasks.\n",
    "The text is available on Researchdata.se as part of [The Arabic E-Book Corpus](https://doi.org/10.5878/7rbh-gy93)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0bd4fb3c2d8976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:18.145949Z",
     "start_time": "2025-12-26T20:32:18.087151Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"مغامرة-العميل-المرموق.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/Abbazone/\"\n",
    "        \"llm-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"العميل.txt\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440af4e8f80f5129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:18.699350Z",
     "start_time": "2025-12-26T20:32:18.694176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 44740\n",
      "مغامرة العميل المرموق\n",
      "\n",
      "تأليف\n",
      "آرثر كونان دويل\n",
      "\n",
      "ترجمة\n",
      "دينا عادل غراب\n",
      "\n",
      "مراجعة\n",
      "شيماء طه الريدي\n",
      "\n",
      "مغامرة العميل المرموق\n",
      "\n",
      "حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.» لأحصل بذلك أخيرًا على الإذن بتدوينِ ما كان — من بضع نواحٍ — اللحظةَ الأبرز والأهم في مسيرة صديقي المهنية ذات يوم.\n",
      "\n",
      "كان لدينا، أنا وهولمز ضَعْفٌ تجاهَ الحَمَّام التركي؛ فلم أجده أقلَّ تحفظًا وأكثر آدميةً كما كان وسطَ البخار في أجواء التراخي الممتعة في حجرة التجفيف\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(f'Total number of characters: {len(raw_text)}')\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb786ef419201f8",
   "metadata": {},
   "source": [
    "Now let's apply our basic tokenizer to the main text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5effd77561e01657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:19.841010Z",
     "start_time": "2025-12-26T20:32:19.833135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the text: 9399\n",
      "Number of unique tokens in the text: 3890\n",
      "First 30 tokens in the text:\n",
      "['مغامرة', 'العميل', 'المرموق', 'تأليف', 'آرثر', 'كونان', 'دويل', 'ترجمة', 'دينا', 'عادل', 'غراب', 'مراجعة', 'شيماء', 'طه', 'الريدي', 'مغامرة', 'العميل', 'المرموق', 'حين', 'طلبتُ', 'الإذنَ', 'من', 'السيد', 'هولمز', '،', 'للمرة', 'العاشرة', 'خلال', 'عدة', 'سنوات', '،', 'للبوح', 'بالقصة', 'التالية', '،', 'أجابني', 'بقوله', ':', '«', 'لا', 'ضررَ', 'من', 'ذلك', 'الآن', '.', '»', 'لأحصل', 'بذلك', 'أخيرًا', 'على', 'الإذن', 'بتدوينِ', 'ما', 'كان', '—', 'من', 'بضع', 'نواحٍ', '—', 'اللحظةَ', 'الأبرز', 'والأهم', 'في', 'مسيرة', 'صديقي', 'المهنية', 'ذات', 'يوم', '.', 'كان', 'لدينا', '،', 'أنا', 'وهولمز', 'ضَعْفٌ', 'تجاهَ', 'الحَمَّام', 'التركي', '؛', 'فلم', 'أجده', 'أقلَّ', 'تحفظًا', 'وأكثر', 'آدميةً', 'كما', 'كان', 'وسطَ', 'البخار', 'في', 'أجواء', 'التراخي', 'الممتعة', 'في', 'حجرة', 'التجفيف', '.', 'يوجد', 'في', 'الدور']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(RE_ENCODER, raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f'Number of tokens in the text: {len(preprocessed)}')\n",
    "print(f'Number of unique tokens in the text: {len(set(preprocessed))}')\n",
    "print(f'First 30 tokens in the text:\\n{preprocessed[:100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d511c7a8af661e",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into IDs\n",
    "\n",
    "Next let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
    "\n",
    "To do this we need to build a vocabulary. This defines how we map each unique token to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0139b150ccdecd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:20.892777Z",
     "start_time": "2025-12-26T20:32:20.887758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3890\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d46d483b5662efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:21.405892Z",
     "start_time": "2025-12-26T20:32:21.399604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('!', 0)\n",
      "1: ('.', 1)\n",
      "2: (':', 2)\n",
      "3: ('«', 3)\n",
      "4: ('»', 4)\n",
      "5: ('،', 5)\n",
      "6: ('؛', 6)\n",
      "7: ('؟', 7)\n",
      "8: ('آبَهُ', 8)\n",
      "9: ('آتٍ', 9)\n",
      "10: ('آثار', 10)\n",
      "11: ('آجلًا', 11)\n",
      "12: ('آخر', 12)\n",
      "13: ('آخرون', 13)\n",
      "14: ('آخرين', 14)\n",
      "15: ('آخِر', 15)\n",
      "16: ('آدميةً', 16)\n",
      "17: ('آرثر', 17)\n",
      "18: ('آلت', 18)\n",
      "19: ('آن', 19)\n",
      "20: ('آنسة', 20)\n",
      "21: ('آه', 21)\n",
      "22: ('أبديت', 22)\n",
      "23: ('أبراج', 23)\n",
      "24: ('أبرهن', 24)\n",
      "25: ('أبوها', 25)\n",
      "26: ('أبي', 26)\n",
      "27: ('أبيها', 27)\n",
      "28: ('أتابع', 28)\n",
      "29: ('أتابِعَ', 29)\n",
      "30: ('أتاح', 30)\n",
      "31: ('أتحدث', 31)\n",
      "32: ('أتحرى', 32)\n",
      "33: ('أتخيل', 33)\n",
      "34: ('أتسألني', 34)\n",
      "35: ('أتعابَك', 35)\n",
      "36: ('أتقصد', 36)\n",
      "37: ('أتوقَّع', 37)\n",
      "38: ('أتيت', 38)\n",
      "39: ('أتينا', 39)\n",
      "40: ('أثارت', 40)\n",
      "41: ('أثر', 41)\n",
      "42: ('أثره', 42)\n",
      "43: ('أثرٍ', 43)\n",
      "44: ('أثرِ', 44)\n",
      "45: ('أجابني', 45)\n",
      "46: ('أجبته', 46)\n",
      "47: ('أجد', 47)\n",
      "48: ('أجده', 48)\n",
      "49: ('أجدها', 49)\n",
      "50: ('أجرة', 50)\n"
     ]
    }
   ],
   "source": [
    "str_to_int = {token: i for i, token in enumerate(all_words)}    #\n",
    "for i, item in enumerate(str_to_int.items()):\n",
    "    print(f'{i}: {item}')\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17232db75593100d",
   "metadata": {},
   "source": [
    "We need also a way to turn token IDs into text. For this we create an inverse version of the vocabulary that maps token IDs back to text tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd42f793c1fe0841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:22.608600Z",
     "start_time": "2025-12-26T20:32:22.605782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أجرة\n"
     ]
    }
   ],
   "source": [
    "int_to_str = {i: s for s, i in str_to_int.items()}\n",
    "print(int_to_str[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ddd711c0bd643",
   "metadata": {},
   "source": [
    "We are now ready to implement a complete tokenizer class in Python with the following features:\n",
    "- an `encode` method that splits text into tokens and carries out the string-to-integer mapping.\n",
    "- a `decode` method that carries out the reverse integer-to-string mapping to convert token IDs back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6cb6865d172ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:24.940673Z",
     "start_time": "2025-12-26T20:32:24.936529Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(RE_ENCODER, text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(RE_DECODER, r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        text = re.sub(r\"«\\s+\", \"«\", text)\n",
    "        text = re.sub(r\"\\s+»\", \"»\", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bfceed9a3c7dc",
   "metadata": {},
   "source": [
    "|We can now use the tokenizer to encode texts into integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6098b085533450ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:26.455024Z",
     "start_time": "2025-12-26T20:32:26.451025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1898, 2382, 524, 3191, 787, 3351, 5, 2919, 869, 1943, 2423, 2187, 5, 2900, 1318, 579, 5, 45, 1446, 2, 3, 2803, 2349, 3191, 2014, 458, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab=str_to_int)\n",
    "text = \"\"\"حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.»\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb73649bbe218",
   "metadata": {},
   "source": [
    "Let's now try to turn these token IDs back into text using the `decode` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8980fcdd5e1fc685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:27.697062Z",
     "start_time": "2025-12-26T20:32:27.693379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.»\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503443c145edd20",
   "metadata": {},
   "source": [
    "Based on this output we can see that the decode method worked successfully.\n",
    "\n",
    "Let's now apply our tokenizer to text sample not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a71ed8b0133f4490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:29.198295Z",
     "start_time": "2025-12-26T20:32:29.152880Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'مرحبا'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mمرحبا، يوم سعيد؟\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(tokenizer.encode(text)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(RE_ENCODER, text)\n\u001b[32m      8\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'مرحبا'"
     ]
    }
   ],
   "source": [
    "text = \"مرحبا، يوم سعيد؟\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6858e10d6dc95",
   "metadata": {},
   "source": [
    "We got `str_to_int['مرحبا']` key error. The problem is that the word \"مرحبا\" was not used in the original text. Hence, it is not contained in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f84cbdf37bcb7",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens\n",
    "\n",
    "We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a models understanding of context or other relevant information of the text.\n",
    "\n",
    "We will modify the vocabulary and tokenizer to support two new tokens `<|unk|>` and `<|endoftext|>`.\n",
    "\n",
    "Let's start by extending our vocabularly with the new tokens `<|unk|>` and `<|endoftext|>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3808c71e54f966ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:33.712461Z",
     "start_time": "2025-12-26T20:32:33.704863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 3890\n",
      "Extended vocabulary size: 3892\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "print(f'Original vocabulary size: {len(str_to_int)}')\n",
    "all_tokens.extend(['<|unk|>', '<|endoftext|>'])\n",
    "str_to_int = {s: i for i, s in enumerate(all_tokens)}\n",
    "print(f'Extended vocabulary size: {len(str_to_int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ddbd9033f204fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:38.007681Z",
     "start_time": "2025-12-26T20:32:37.991910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—: 3887\n",
      "…: 3888\n",
      "ﻟ: 3889\n",
      "<|unk|>: 3890\n",
      "<|endoftext|>: 3891\n"
     ]
    }
   ],
   "source": [
    "for s, i in list(str_to_int.items())[-5:]:\n",
    "    print(f'{s}: {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d7ec5f3a7e65b",
   "metadata": {},
   "source": [
    "|We confirm that the two new special tokens were successfully incorporated into the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712c8bca3c90b56",
   "metadata": {},
   "source": [
    "We are now ready to implement a complete tokenizer class in Python with the following features:\n",
    "- an `encode` method that splits text into tokens and carries out the string-to-integer mapping.\n",
    "- unknown words that are not part of the vocabulary must be mapped to special token `<|unk|>`.\n",
    "- independent text sources must be separated by special token `<|endoftext|>`.\n",
    "- a `decode` method that carries out the reverse integer-to-string mapping to convert token IDs back to text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96f520683694ad5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:44.969904Z",
     "start_time": "2025-12-26T20:32:44.963809Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(RE_ENCODER, text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item.strip() if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(RE_DECODER, r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        text = re.sub(r\"«\\s+\", \"«\", text)\n",
    "        text = re.sub(r\"\\s+»\", \"»\", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c4596ea69961a",
   "metadata": {},
   "source": [
    "Let's now apply the tokenizer to a new text sample not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c76c2e8919e3efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:47.344736Z",
     "start_time": "2025-12-26T20:32:47.338504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3890, 5, 3852, 2172, 7]\n",
      "<|unk|>، يوم سعيد؟\n"
     ]
    }
   ],
   "source": [
    "text = \"مرحبا، يوم سعيد؟\"\n",
    "tokenizer = SimpleTokenizerV2(vocab=str_to_int)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00ab19e9a1e7a3",
   "metadata": {},
   "source": [
    "The first word مرحبا has successfully been mapped to token ID 3890 and back to `<|unk|>`.\n",
    "\n",
    "Let's now try to combine two independent texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d8860ed2c3d2464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:44:45.440689Z",
     "start_time": "2025-12-26T11:44:45.430996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "text1 = \"مرحبا، يوم سعيد؟\"\n",
    "text2 = \"حين طلبتُ، كلمةجديدة، من السيد هولمز\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42d90c234103c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:41:20.514773Z",
     "start_time": "2025-12-26T11:41:20.509894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3890, 5, 3852, 2172, 7, 3891, 1898, 2382, 5, 3890, 5, 3191, 787, 3351]\n",
      "<|unk|>، يوم سعيد؟ <|endoftext|> حين طلبتُ، <|unk|>، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2351c4a266aec91",
   "metadata": {},
   "source": [
    "## 2.5 Byte pair encoding\n",
    "\n",
    "The Byte Pair Encoder (BPE) was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "\n",
    "Let's first look at an existing implementation from the tiktoken library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "805eb2c0eea95a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:34:26.473988Z",
     "start_time": "2025-12-26T20:34:26.286655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "text1 = \"مرحبا، يوم سعيد؟\"\n",
    "text2 = \"حين طلبتُ، كلمةجديدة، من السيد هولمز\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32531b645e1a1268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:46:36.911092Z",
     "start_time": "2025-12-26T20:46:36.902170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25405, 26897, 148, 255, 39848, 12919, 148, 234, 18923, 232, 30335, 25405, 17550, 111, 44690, 22654, 38843, 148, 253, 220, 50256, 17550, 255, 22654, 23338, 17550, 115, 13862, 39848, 41486, 149, 237, 148, 234, 18923, 225, 13862, 25405, 45632, 148, 105, 38843, 22654, 38843, 45632, 148, 234, 47048, 23338, 28981, 45692, 22654, 38843, 18923, 229, 30335, 13862, 25405, 148, 110]\n",
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc0dbb7765f7cb",
   "metadata": {},
   "source": [
    "The encoding and decoding looks good.\n",
    "\n",
    "Specifically, we see that the BPE tokenizer managed to encode and decode unknown words such as كلمةجديدة correctly.\n",
    "\n",
    "This is because the algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42cf9a7153241fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T21:07:28.369819Z",
     "start_time": "2025-12-26T21:07:28.350728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25405, 26897, 148, 255, 39848, 12919]\n",
      "[149, 225, 13862, 25405, 45632, 148, 105, 38843, 22654, 38843, 45632]\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('مرحبا'))\n",
    "print(tokenizer.encode('كلمةجديدة'))\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337d3c61f52ebe8",
   "metadata": {},
   "source": [
    "But notice also the large number of token IDs compared to the previous approach based on `re.split`.\n",
    "\n",
    "To understand why, let's take a closer look at how the GPT-2 tokenization works:\n",
    "1. Convert the text to UTF-8 bytes\n",
    "2. Map bytes through a reversible “byte encoder”\n",
    "3. Iteratively apply BPE merges to combine frequent byte sequences into tokens\n",
    "\n",
    "For English text, many common sequences have merges, so you get big tokens like \" hello\" or \"ing\".\n",
    "\n",
    "For Arabic (and many non-Latin scripts), GPT-2’s merges are much weaker because the GPT-2 vocab was built from data that was heavily skewed toward English/Latin text. So Arabic often falls back to smaller byte chunks, meaning more tokens. Furthermore, the presence of diacritics such as Tashkil (تشكيل) or Harakat (حركات) for vowels, like for example in طلبتُ which includes damma/tanween, is treated as a separate Unicode point. This often breaks BPE merges which leads to more tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
