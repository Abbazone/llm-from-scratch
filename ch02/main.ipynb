{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Working with text data\n",
    "\n",
    "Here we will see how to prepare input text for training LLMs. This involves splitting the text into individual word and subword tokens, which can be encoded into vector representations for the LLM."
   ],
   "id": "914f14b64115aded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1 Understanding word embeddings\n",
    "\n",
    "Deep neural network models, including LLMs, cannot process raw text directly. Therefore, we need a way to represent words as continous-valued vectors.\n",
    "\n",
    "The concept of converting data into a vector format is often referred to as embedding.\n",
    "\n",
    "Word embeddings can have varying dimensions, from one to thousands."
   ],
   "id": "c99f31a3d1879071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Tokenizing text\n",
    "\n",
    "Let's see how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM.\n",
    "\n",
    "We start with a simple text and Pythonâ€™s `re.split` function to split the text while keeping the delimiters:"
   ],
   "id": "9650dbd08cb40ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T11:10:08.959282Z",
     "start_time": "2025-12-24T11:10:08.903802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)    # capturing group (...) with three alternatives, single characters [...], -- and whitespace\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "365e647f4bd7231b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T10:44:47.864941Z",
     "start_time": "2025-12-24T10:44:47.149051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ],
   "id": "6c0bd4fb3c2d8976",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:15:59.907483Z",
     "start_time": "2025-12-24T10:15:59.867218Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(f'Total number of characters: {len(raw_text)}')\n",
    "print(raw_text[:99])"
   ],
   "id": "440af4e8f80f5129"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's apply our basic tokenizer to the main text:",
   "id": "dfb786ef419201f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T17:20:59.052363Z",
     "start_time": "2025-12-24T17:20:56.012201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f'Number of tokens in the text: {len(preprocessed)}')\n",
    "print(f'Number of unique tokens in the text: {len(set(preprocessed))}')\n",
    "print(f'First 30 tokens in the text:\\n{preprocessed[:30]}')"
   ],
   "id": "5effd77561e01657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the text: 4690\n",
      "Number of unique tokens in the text: 1130\n",
      "First 30 tokens in the text:\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3 Converting tokens into IDs\n",
    "\n",
    "Next let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
    "\n",
    "To do this we need to build a vocabulary. This defines how we map each unique token to a unique integer."
   ],
   "id": "36d511c7a8af661e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T17:22:19.307232Z",
     "start_time": "2025-12-24T17:22:16.271058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ],
   "id": "e0139b150ccdecd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T17:34:59.074269Z",
     "start_time": "2025-12-24T17:34:56.061719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "str_to_int = {token: i for i, token in enumerate(all_words)}    #\n",
    "for i, item in enumerate(str_to_int.items()):\n",
    "    print(f'{i}: {item}')\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "8d46d483b5662efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('!', 0)\n",
      "1: ('\"', 1)\n",
      "2: (\"'\", 2)\n",
      "3: ('(', 3)\n",
      "4: (')', 4)\n",
      "5: (',', 5)\n",
      "6: ('--', 6)\n",
      "7: ('.', 7)\n",
      "8: (':', 8)\n",
      "9: (';', 9)\n",
      "10: ('?', 10)\n",
      "11: ('A', 11)\n",
      "12: ('Ah', 12)\n",
      "13: ('Among', 13)\n",
      "14: ('And', 14)\n",
      "15: ('Are', 15)\n",
      "16: ('Arrt', 16)\n",
      "17: ('As', 17)\n",
      "18: ('At', 18)\n",
      "19: ('Be', 19)\n",
      "20: ('Begin', 20)\n",
      "21: ('Burlington', 21)\n",
      "22: ('But', 22)\n",
      "23: ('By', 23)\n",
      "24: ('Carlo', 24)\n",
      "25: ('Chicago', 25)\n",
      "26: ('Claude', 26)\n",
      "27: ('Come', 27)\n",
      "28: ('Croft', 28)\n",
      "29: ('Destroyed', 29)\n",
      "30: ('Devonshire', 30)\n",
      "31: ('Don', 31)\n",
      "32: ('Dubarry', 32)\n",
      "33: ('Emperors', 33)\n",
      "34: ('Florence', 34)\n",
      "35: ('For', 35)\n",
      "36: ('Gallery', 36)\n",
      "37: ('Gideon', 37)\n",
      "38: ('Gisburn', 38)\n",
      "39: ('Gisburns', 39)\n",
      "40: ('Grafton', 40)\n",
      "41: ('Greek', 41)\n",
      "42: ('Grindle', 42)\n",
      "43: ('Grindles', 43)\n",
      "44: ('HAD', 44)\n",
      "45: ('Had', 45)\n",
      "46: ('Hang', 46)\n",
      "47: ('Has', 47)\n",
      "48: ('He', 48)\n",
      "49: ('Her', 49)\n",
      "50: ('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need also a way to turn token IDs into text. For this we create an inverse version of the vocabulary that maps token IDs back to text tokens:",
   "id": "17232db75593100d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "int_to_str = {i: s for s, i in str_to_int.items()}",
   "id": "dd42f793c1fe0841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf5b54cce0364a7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
