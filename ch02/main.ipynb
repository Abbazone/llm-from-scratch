{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Working with text data\n",
    "\n",
    "Here we will see how to prepare input text for training LLMs. This involves splitting the text into individual word and subword tokens, which can be encoded into vector representations for the LLM."
   ],
   "id": "914f14b64115aded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1 Understanding word embeddings\n",
    "\n",
    "Deep neural network models, including LLMs, cannot process raw text directly. Therefore, we need a way to represent words as continous-valued vectors.\n",
    "\n",
    "The concept of converting data into a vector format is often referred to as embedding.\n",
    "\n",
    "Word embeddings can have varying dimensions, from one to thousands."
   ],
   "id": "c99f31a3d1879071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Tokenizing text\n",
    "\n",
    "Let's see how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM.\n",
    "\n",
    "We start with a simple text and Pythonâ€™s `re.split` function to split the text while keeping the delimiters:"
   ],
   "id": "9650dbd08cb40ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:20.771103Z",
     "start_time": "2025-12-24T18:13:20.726715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)    # capturing group (...) with three alternatives, single characters [...], -- and whitespace\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "365e647f4bd7231b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:21.126905Z",
     "start_time": "2025-12-24T18:13:21.064545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ],
   "id": "6c0bd4fb3c2d8976",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:21.718691Z",
     "start_time": "2025-12-24T18:13:21.697910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(f'Total number of characters: {len(raw_text)}')\n",
    "print(raw_text[:99])"
   ],
   "id": "440af4e8f80f5129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's apply our basic tokenizer to the main text:",
   "id": "dfb786ef419201f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:22.881751Z",
     "start_time": "2025-12-24T18:13:22.854018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f'Number of tokens in the text: {len(preprocessed)}')\n",
    "print(f'Number of unique tokens in the text: {len(set(preprocessed))}')\n",
    "print(f'First 30 tokens in the text:\\n{preprocessed[:30]}')"
   ],
   "id": "5effd77561e01657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the text: 4690\n",
      "Number of unique tokens in the text: 1130\n",
      "First 30 tokens in the text:\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3 Converting tokens into IDs\n",
    "\n",
    "Next let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
    "\n",
    "To do this we need to build a vocabulary. This defines how we map each unique token to a unique integer."
   ],
   "id": "36d511c7a8af661e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:24.164767Z",
     "start_time": "2025-12-24T18:13:24.150665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ],
   "id": "e0139b150ccdecd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:25.015579Z",
     "start_time": "2025-12-24T18:13:24.985326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "str_to_int = {token: i for i, token in enumerate(all_words)}    #\n",
    "for i, item in enumerate(str_to_int.items()):\n",
    "    print(f'{i}: {item}')\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "8d46d483b5662efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('!', 0)\n",
      "1: ('\"', 1)\n",
      "2: (\"'\", 2)\n",
      "3: ('(', 3)\n",
      "4: (')', 4)\n",
      "5: (',', 5)\n",
      "6: ('--', 6)\n",
      "7: ('.', 7)\n",
      "8: (':', 8)\n",
      "9: (';', 9)\n",
      "10: ('?', 10)\n",
      "11: ('A', 11)\n",
      "12: ('Ah', 12)\n",
      "13: ('Among', 13)\n",
      "14: ('And', 14)\n",
      "15: ('Are', 15)\n",
      "16: ('Arrt', 16)\n",
      "17: ('As', 17)\n",
      "18: ('At', 18)\n",
      "19: ('Be', 19)\n",
      "20: ('Begin', 20)\n",
      "21: ('Burlington', 21)\n",
      "22: ('But', 22)\n",
      "23: ('By', 23)\n",
      "24: ('Carlo', 24)\n",
      "25: ('Chicago', 25)\n",
      "26: ('Claude', 26)\n",
      "27: ('Come', 27)\n",
      "28: ('Croft', 28)\n",
      "29: ('Destroyed', 29)\n",
      "30: ('Devonshire', 30)\n",
      "31: ('Don', 31)\n",
      "32: ('Dubarry', 32)\n",
      "33: ('Emperors', 33)\n",
      "34: ('Florence', 34)\n",
      "35: ('For', 35)\n",
      "36: ('Gallery', 36)\n",
      "37: ('Gideon', 37)\n",
      "38: ('Gisburn', 38)\n",
      "39: ('Gisburns', 39)\n",
      "40: ('Grafton', 40)\n",
      "41: ('Greek', 41)\n",
      "42: ('Grindle', 42)\n",
      "43: ('Grindles', 43)\n",
      "44: ('HAD', 44)\n",
      "45: ('Had', 45)\n",
      "46: ('Hang', 46)\n",
      "47: ('Has', 47)\n",
      "48: ('He', 48)\n",
      "49: ('Her', 49)\n",
      "50: ('Hermia', 50)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need also a way to turn token IDs into text. For this we create an inverse version of the vocabulary that maps token IDs back to text tokens:",
   "id": "17232db75593100d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T18:13:27.172377Z",
     "start_time": "2025-12-24T18:13:27.154843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "int_to_str = {i: s for s, i in str_to_int.items()}\n",
    "print(int_to_str[50])"
   ],
   "id": "dd42f793c1fe0841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hermia\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We are now ready to implement a complete tokenizer class in Python with the following features:\n",
    "- an `encode` method that splits text into tokens and carries out the string-to-integer mapping.\n",
    "- unknown words that are not part of the vocabulary must be mapped to special token `<|unk|>`.\n",
    "- independent text sources must be separated by special token `<|endoftext|>`.\n",
    "- a `decode` method that carries out the reverse integer-to-string mapping to convert token IDs back to text.\n",
    "\n",
    "Let's start by extending our vocabularly with the new tokens `<|unk|>` and `<|endoftext|>`:"
   ],
   "id": "9fbc9e78f3f01ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:23:16.443480Z",
     "start_time": "2025-12-24T20:23:16.395100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend(['<|unk|>', '<|endoftext|>'])\n",
    "str_to_int = {s: i for i, s in enumerate(all_tokens)}\n",
    "print(f'Vocabulary size: {len(str_to_int)}')"
   ],
   "id": "3808c71e54f966ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1132\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:26:22.651028Z",
     "start_time": "2025-12-24T20:26:22.600361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s, i in list(str_to_int.items())[-5:]:\n",
    "    print(f'{i}: {s}')"
   ],
   "id": "55ddbd9033f204fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127: younger\n",
      "1128: your\n",
      "1129: yourself\n",
      "1130: <|unk|>\n",
      "1131: <|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We confirm that the two new special tokens were successfully incorporated into the vocabulary.",
   "id": "f86d7ec5f3a7e65b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:23:52.710232Z",
     "start_time": "2025-12-24T20:23:52.689437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item.strip() if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        return text\n"
   ],
   "id": "96f520683694ad5a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:23:53.403698Z",
     "start_time": "2025-12-24T20:23:53.371395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizer(vocab=str_to_int)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "id": "475e5019b0e9dadf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's try to convert these token IDs back into text using the decode method:",
   "id": "6045dcd1d384f690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:24:00.732341Z",
     "start_time": "2025-12-24T20:24:00.690586Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(ids))",
   "id": "7d2c11448019ab5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This looks good. Next, let's now apply the tokenizer to a new text sample not contained in the training set:",
   "id": "542c4596ea69961a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:33:27.652502Z",
     "start_time": "2025-12-24T20:33:27.589068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "4c76c2e8919e3efe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 355, 1126, 628, 975, 10]\n",
      "<|unk|>, do you like tea?\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first word Hello has successfully been mapped to token ID 1130 and back to `<|unk|>`.\n",
    "\n",
    "Let's now try to combine two independent texts:"
   ],
   "id": "2b00ab19e9a1e7a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:35:21.802108Z",
     "start_time": "2025-12-24T20:35:21.758924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ],
   "id": "8d8860ed2c3d2464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:35:46.930027Z",
     "start_time": "2025-12-24T20:35:46.889491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "42d90c234103c80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.5 Byte pair encoding\n",
    "\n",
    "The Byte Pair Encoder (BPE) was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "\n",
    "Let's first look at an existing implementation from the tiktoken library:"
   ],
   "id": "e2351c4a266aec91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:02:17.031213Z",
     "start_time": "2025-12-24T21:02:17.014448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\""
   ],
   "id": "805eb2c0eea95a70",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:02:22.220543Z",
     "start_time": "2025-12-24T21:02:22.172439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ],
   "id": "32531b645e1a1268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The encoding and decoding looks good.\n",
    "\n",
    "Specifically, we see that the BPE tokenizer managed to encode and decode unknown words such as someunknowPlace correctly.\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabularly words.\n",
    "\n",
    "Let's take a closer look:"
   ],
   "id": "d5fc0dbb7765f7cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:05:12.709957Z",
     "start_time": "2025-12-24T21:05:12.657726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.encode('someunknownPlace')\n",
    "print(tokens)"
   ],
   "id": "3ab5d78bc07906e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11246, 34680, 27271]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T21:05:13.247445Z",
     "start_time": "2025-12-24T21:05:13.222054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in tokens:\n",
    "    print(token, tokenizer.decode([token]))"
   ],
   "id": "2bd59b8df3bf253c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11246 some\n",
      "34680 unknown\n",
      "27271 Place\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "How does it do this? BPE build its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words.",
   "id": "b786e206228eb0c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenizer.decode([11246])",
   "id": "6050f07a47e29f21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
