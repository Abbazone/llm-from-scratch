{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914f14b64115aded",
   "metadata": {},
   "source": [
    "# التعامل مع النصوص #\n",
    "\n",
    "سوف نستعرض هنا كيفية تحضير النصوص العربية لاستخدامها في تدريب نماذج اللغة الكبيرة.\n",
    "\n",
    "سوف نقوم في البداية بتحويل النص إلى ترميزات، حيث تمثل كل ترميزة كلمة أو جزء من كلمة. بعد ذلك نقوم بتشفير الترميزات باستخدام متجهات متعددة الأبعاد تسمى تضمينات، يمكن استخدامها لتدريب نماذج اللغة الكبيرة.\n",
    "\n",
    "تجدر الأشارة إلى أن النص العربي يختلف عن غيره من اللغات مثل الإنجليزية بأنه يكتب من اليمين إلى اليسار و باحتوائه على الحركات أو التشكيل، مما يتطلب التعامل معه بشكل مختلف في بعض الأمور."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f31a3d1879071",
   "metadata": {},
   "source": [
    " ## تضمينات الكلمات\n",
    "يتعذر على نماذج التعليم العميق، بما في ذلك نماذج اللغة الكبيرة، من معالجة النصوص بشكل مباشر. لذلك يجب علينا تمثيل الكلمات كمتجهات متعددة الأبعاد تسمى تضمينات، ويُشار إلى هذه العملية باسم تضمين الكلمات."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650dbd08cb40ce",
   "metadata": {},
   "source": [
    "\n",
    " ## ترميز النص\n",
    "للبدء بتضمين الكلمات، يجب علينا أولا تقسيم النص إلى أجزاء تسمى ترميزات.\n",
    "\n",
    "لنرى كيف يمكن تجزئة جملة قصيرة باستخدام الدالية  `re.split` مع الحفاظ على الفواصل:"
   ]
  },
  {
   "cell_type": "code",
   "id": "365e647f4bd7231b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:00:49.371164Z",
     "start_time": "2025-12-29T21:00:49.309057Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "RE_ENCODER = r'([،.؟!:؛«»—]|\\s)'\n",
    "RE_DECODER = r'\\s+([،.؟!:؛—])'\n",
    "\n",
    "text = \"مرحبا، بالعالم. هل هذا— اختبار؟\"\n",
    "result = re.split(RE_ENCODER, text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n",
    "print(result[0])\n",
    "print(result[-1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مرحبا', '،', 'بالعالم', '.', 'هل', 'هذا', '—', 'اختبار', '؟']\n",
      "مرحبا\n",
      "؟\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "60ab64420a72dca5",
   "metadata": {},
   "source": [
    "لاحظ كيف يظهر النص العربي من اليمين إلى اليسار، بينما يتم تخزين السلسلة بالترتيب الذي تمت كتابتها به.\n",
    "\n",
    "قي Python، `str` عبارة عن تسلسل من نقاط رموز Unicode مخزنة بالترتيب الذي تمت كتابتها به (الترتيب المنطقي)، لذا فإن `result[0] = مرحبا` و`result[-1] = ؟` بغض النظر عما إذا كان يتم عرضها من اليمين إلى اليسار أو من اليسار إلى اليمين.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1abb6603663640",
   "metadata": {},
   "source": [
    "\n",
    "النص الذي سنقوم بتحويله إلى رموز لتدريب LLM هو ”مغامرة العميل المرموق“، وهو متوفر في المجال العام وبالتالي يُسمح باستخدامه في مهام تدريب LLM.\n",
    "\n",
    "النص متاح على Researchdata.se كجزء من [مجموعة الكتب الإلكترونية العربية](https://doi.org/10.5878/7rbh-gy93).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c0bd4fb3c2d8976",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:08.507080Z",
     "start_time": "2025-12-29T21:03:08.426414Z"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"مغامرة-العميل-المرموق.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/Abbazone/\"\n",
    "        \"llm-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"العميل.txt\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "440af4e8f80f5129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:09.179641Z",
     "start_time": "2025-12-29T21:03:09.158153Z"
    }
   },
   "source": [
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(f'Total number of characters: {len(raw_text)}')\n",
    "print(raw_text[:500])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 44740\n",
      "مغامرة العميل المرموق\n",
      "\n",
      "تأليف\n",
      "آرثر كونان دويل\n",
      "\n",
      "ترجمة\n",
      "دينا عادل غراب\n",
      "\n",
      "مراجعة\n",
      "شيماء طه الريدي\n",
      "\n",
      "مغامرة العميل المرموق\n",
      "\n",
      "حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.» لأحصل بذلك أخيرًا على الإذن بتدوينِ ما كان — من بضع نواحٍ — اللحظةَ الأبرز والأهم في مسيرة صديقي المهنية ذات يوم.\n",
      "\n",
      "كان لدينا، أنا وهولمز ضَعْفٌ تجاهَ الحَمَّام التركي؛ فلم أجده أقلَّ تحفظًا وأكثر آدميةً كما كان وسطَ البخار في أجواء التراخي الممتعة في حجرة التجفيف\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "dfb786ef419201f8",
   "metadata": {},
   "source": "لنختبر المرمز الذي كتبناه على هذا النص الكبير نسبيا:"
  },
  {
   "cell_type": "code",
   "id": "5effd77561e01657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:12.190934Z",
     "start_time": "2025-12-29T21:03:12.123844Z"
    }
   },
   "source": [
    "preprocessed = re.split(RE_ENCODER, raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f'Number of tokens in the text: {len(preprocessed)}')\n",
    "print(f'Number of unique tokens in the text: {len(set(preprocessed))}')\n",
    "print(f'First 30 tokens in the text:\\n{preprocessed[:100]}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the text: 9399\n",
      "Number of unique tokens in the text: 3890\n",
      "First 30 tokens in the text:\n",
      "['مغامرة', 'العميل', 'المرموق', 'تأليف', 'آرثر', 'كونان', 'دويل', 'ترجمة', 'دينا', 'عادل', 'غراب', 'مراجعة', 'شيماء', 'طه', 'الريدي', 'مغامرة', 'العميل', 'المرموق', 'حين', 'طلبتُ', 'الإذنَ', 'من', 'السيد', 'هولمز', '،', 'للمرة', 'العاشرة', 'خلال', 'عدة', 'سنوات', '،', 'للبوح', 'بالقصة', 'التالية', '،', 'أجابني', 'بقوله', ':', '«', 'لا', 'ضررَ', 'من', 'ذلك', 'الآن', '.', '»', 'لأحصل', 'بذلك', 'أخيرًا', 'على', 'الإذن', 'بتدوينِ', 'ما', 'كان', '—', 'من', 'بضع', 'نواحٍ', '—', 'اللحظةَ', 'الأبرز', 'والأهم', 'في', 'مسيرة', 'صديقي', 'المهنية', 'ذات', 'يوم', '.', 'كان', 'لدينا', '،', 'أنا', 'وهولمز', 'ضَعْفٌ', 'تجاهَ', 'الحَمَّام', 'التركي', '؛', 'فلم', 'أجده', 'أقلَّ', 'تحفظًا', 'وأكثر', 'آدميةً', 'كما', 'كان', 'وسطَ', 'البخار', 'في', 'أجواء', 'التراخي', 'الممتعة', 'في', 'حجرة', 'التجفيف', '.', 'يوجد', 'في', 'الدور']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "36d511c7a8af661e",
   "metadata": {},
   "source": [
    "\n",
    "## تحويل الترميزات إلى معرفات\n",
    "\n",
    "في هذه الخطوة سوف نقوم بتحويل الترميزات إلى أعداد تسمي معرفات الترميزات. يتم ذلك من خلال إنشاء مفردات تحدد كيفية تعيين كل ترميزه إلى عدد صحيح فريد.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0139b150ccdecd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:13.759512Z",
     "start_time": "2025-12-29T21:03:13.706365Z"
    }
   },
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3890\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8d46d483b5662efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:15.309435Z",
     "start_time": "2025-12-29T21:03:15.261148Z"
    }
   },
   "source": [
    "str_to_int = {token: i for i, token in enumerate(all_words)}    #\n",
    "for i, item in enumerate(str_to_int.items()):\n",
    "    print(f'{i}: {item}')\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('!', 0)\n",
      "1: ('.', 1)\n",
      "2: (':', 2)\n",
      "3: ('«', 3)\n",
      "4: ('»', 4)\n",
      "5: ('،', 5)\n",
      "6: ('؛', 6)\n",
      "7: ('؟', 7)\n",
      "8: ('آبَهُ', 8)\n",
      "9: ('آتٍ', 9)\n",
      "10: ('آثار', 10)\n",
      "11: ('آجلًا', 11)\n",
      "12: ('آخر', 12)\n",
      "13: ('آخرون', 13)\n",
      "14: ('آخرين', 14)\n",
      "15: ('آخِر', 15)\n",
      "16: ('آدميةً', 16)\n",
      "17: ('آرثر', 17)\n",
      "18: ('آلت', 18)\n",
      "19: ('آن', 19)\n",
      "20: ('آنسة', 20)\n",
      "21: ('آه', 21)\n",
      "22: ('أبديت', 22)\n",
      "23: ('أبراج', 23)\n",
      "24: ('أبرهن', 24)\n",
      "25: ('أبوها', 25)\n",
      "26: ('أبي', 26)\n",
      "27: ('أبيها', 27)\n",
      "28: ('أتابع', 28)\n",
      "29: ('أتابِعَ', 29)\n",
      "30: ('أتاح', 30)\n",
      "31: ('أتحدث', 31)\n",
      "32: ('أتحرى', 32)\n",
      "33: ('أتخيل', 33)\n",
      "34: ('أتسألني', 34)\n",
      "35: ('أتعابَك', 35)\n",
      "36: ('أتقصد', 36)\n",
      "37: ('أتوقَّع', 37)\n",
      "38: ('أتيت', 38)\n",
      "39: ('أتينا', 39)\n",
      "40: ('أثارت', 40)\n",
      "41: ('أثر', 41)\n",
      "42: ('أثره', 42)\n",
      "43: ('أثرٍ', 43)\n",
      "44: ('أثرِ', 44)\n",
      "45: ('أجابني', 45)\n",
      "46: ('أجبته', 46)\n",
      "47: ('أجد', 47)\n",
      "48: ('أجده', 48)\n",
      "49: ('أجدها', 49)\n",
      "50: ('أجرة', 50)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "17232db75593100d",
   "metadata": {},
   "source": "بالمقابل نحتاج أيضا إلى طريقة لتحويل معرفات الترميزات إلى الترميزات الاصلية:"
  },
  {
   "cell_type": "code",
   "id": "dd42f793c1fe0841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:16.934423Z",
     "start_time": "2025-12-29T21:03:16.906041Z"
    }
   },
   "source": [
    "int_to_str = {i: s for s, i in str_to_int.items()}\n",
    "print(int_to_str[50])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أجرة\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b65ddd711c0bd643",
   "metadata": {},
   "source": [
    "\n",
    "سنقوم الآن باستخدام فئة بايثون لكتابة مرمز متكامل بالمميزات التالية:\n",
    "\n",
    "- دالية التشفير: تقوم بتقسيم النص إلى ترميزات ومن ثم تحويلها إلى معرفات.\n",
    "\n",
    " - دالية فك التشفير: تعمل بالعكس، أي تقوم بتحويل المعرفات إلى ترميزات مرة أخرى.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e6cb6865d172ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:18.576134Z",
     "start_time": "2025-12-29T21:03:18.559242Z"
    }
   },
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(RE_ENCODER, text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(RE_DECODER, r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        text = re.sub(r\"«\\s+\", \"«\", text)\n",
    "        text = re.sub(r\"\\s+»\", \"»\", text)\n",
    "        return text\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "3d0bfceed9a3c7dc",
   "metadata": {},
   "source": "لنختبر دالية التشفير في المرمز لتحويل النص إلى أعداد:"
  },
  {
   "cell_type": "code",
   "id": "6098b085533450ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:24.432933Z",
     "start_time": "2025-12-29T21:03:24.391184Z"
    }
   },
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab=str_to_int)\n",
    "text = \"\"\"حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.»\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1898, 2382, 524, 3191, 787, 3351, 5, 2919, 869, 1943, 2423, 2187, 5, 2900, 1318, 579, 5, 45, 1446, 2, 3, 2803, 2349, 3191, 2014, 458, 1, 4]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "4edb73649bbe218",
   "metadata": {},
   "source": [
    "\n",
    "ثم لنختبر دالية فك التشفير لتحويل الأعداد إلى ترميزات:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8980fcdd5e1fc685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:26.283493Z",
     "start_time": "2025-12-29T21:03:26.250374Z"
    }
   },
   "source": [
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.»\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "2503443c145edd20",
   "metadata": {},
   "source": [
    "\n",
    "بناء على هذا نستنتج أن عملية التشفير وفك التشفير تمت بنجاح.\n",
    "\n",
    "لكن ماذا يحدث إذا صادف المرمز كلمات جديدة لم تكن موجودة عند بناء المفردات؟ لنتابع المثال التالي؟"
   ]
  },
  {
   "cell_type": "code",
   "id": "a71ed8b0133f4490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:03:28.327938Z",
     "start_time": "2025-12-29T21:03:28.132829Z"
    }
   },
   "source": [
    "text = \"مرحبا، يوم سعيد؟\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'مرحبا'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m text = \u001B[33m\"\u001B[39m\u001B[33mمرحبا، يوم سعيد؟\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(tokenizer.decode(tokenizer.encode(text)))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mSimpleTokenizerV1.encode\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m      7\u001B[39m preprocessed = re.split(RE_ENCODER, text)\n\u001B[32m      8\u001B[39m preprocessed = [item.strip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item.strip()]\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m ids = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[31mKeyError\u001B[39m: 'مرحبا'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "66d6858e10d6dc95",
   "metadata": {},
   "source": "سبب هذا الخطأ هو أن كلمة \"مرحبا\" كلمة جديدة، أي ليست موجودة في النص الأصلي، وبالتالي ليست موجودة في المفردات. سنرى كيف يمكن التغلب على هذه المشكلة."
  },
  {
   "cell_type": "markdown",
   "id": "1c2f84cbdf37bcb7",
   "metadata": {},
   "source": [
    "## إضافة الترميزات الخاصة\n",
    "سنقوم بإضافة بعض التعديلات على المرمز ليتمكن من التعامل مع الكلمات الجديدة. سنضيف أيضا تعديلا يساعد النموذج اللغوي على فهم سياق النص من خلال الفصل بين النصوص المتتابعة.\n",
    "سنقوم بتعديل المفردات والمرمز لدعم ترميزين جديدين هما `<|unk|>` و`<|endoftext|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3808c71e54f966ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:33.712461Z",
     "start_time": "2025-12-26T20:32:33.704863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 3890\n",
      "Extended vocabulary size: 3892\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "print(f'Original vocabulary size: {len(str_to_int)}')\n",
    "all_tokens.extend(['<|unk|>', '<|endoftext|>'])\n",
    "str_to_int = {s: i for i, s in enumerate(all_tokens)}\n",
    "print(f'Extended vocabulary size: {len(str_to_int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ddbd9033f204fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:38.007681Z",
     "start_time": "2025-12-26T20:32:37.991910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—: 3887\n",
      "…: 3888\n",
      "ﻟ: 3889\n",
      "<|unk|>: 3890\n",
      "<|endoftext|>: 3891\n"
     ]
    }
   ],
   "source": [
    "for s, i in list(str_to_int.items())[-5:]:\n",
    "    print(f'{s}: {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d7ec5f3a7e65b",
   "metadata": {},
   "source": "تم دمج الترميزتين الجديدتين في المفردات بنجاح.\n"
  },
  {
   "cell_type": "markdown",
   "id": "a712c8bca3c90b56",
   "metadata": {},
   "source": [
    "سنقوم الآن بإضافة التعديلات التالية على فئة بايثون السابقة:\n",
    "- دالية التشفير: تقوم بتقسيم النص إلى ترميزات ومن ثم تحويلها إلى معرفات.\n",
    "- يتم إرسال الكلمات الجديدة إلى ترميزة خاصة `<|unk|>`\n",
    "- يتم الفصل بين النصوص المختلفة باستخدام ترميزة خاصة `<|endoftext|>`.\n",
    " - دالية فك التشفير: تعمل بالعكس، أي تقوم بتحويل المعرفات إلى ترميزات مرة أخرى.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96f520683694ad5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:44.969904Z",
     "start_time": "2025-12-26T20:32:44.963809Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(RE_ENCODER, text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item.strip() if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(RE_DECODER, r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        text = re.sub(r\"«\\s+\", \"«\", text)\n",
    "        text = re.sub(r\"\\s+»\", \"»\", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c4596ea69961a",
   "metadata": {},
   "source": "لنختبر المرمز المعدل على الجملة السابقة:\n"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c76c2e8919e3efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:47.344736Z",
     "start_time": "2025-12-26T20:32:47.338504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3890, 5, 3852, 2172, 7]\n",
      "<|unk|>، يوم سعيد؟\n"
     ]
    }
   ],
   "source": [
    "text = \"مرحبا، يوم سعيد؟\"\n",
    "tokenizer = SimpleTokenizerV2(vocab=str_to_int)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00ab19e9a1e7a3",
   "metadata": {},
   "source": [
    "نلاحظ أنه تم تعيين الترميزة `<|unk|>` لكلمة \"مرحبا\" بنجاح.\n",
    "\n",
    "لنختبر الآن المرمز على جملتين مختلفتين:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d8860ed2c3d2464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T21:49:37.248355Z",
     "start_time": "2025-12-29T21:49:37.206460Z"
    }
   },
   "source": [
    "text1 = \"مرحبا، يوم سعيد؟\"\n",
    "text2 = \"حين طلبتُ، كلمةجديدة، من السيد هولمز\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42d90c234103c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:41:20.514773Z",
     "start_time": "2025-12-26T11:41:20.509894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3890, 5, 3852, 2172, 7, 3891, 1898, 2382, 5, 3890, 5, 3191, 787, 3351]\n",
      "<|unk|>، يوم سعيد؟ <|endoftext|> حين طلبتُ، <|unk|>، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "نرى أن المرمز تعرف على الكلمات الجديدة مثل \"مرحبا\" و \"كلمةجديدة\"، وأيضا الترميزة الخاصة `<|endoftext|>` بنجاح.",
   "id": "918723d6188245de"
  },
  {
   "cell_type": "markdown",
   "id": "e2351c4a266aec91",
   "metadata": {},
   "source": [
    "## 2.5 Byte pair encoding\n",
    "\n",
    "The Byte Pair Encoder (BPE) was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "\n",
    "Let's first look at an existing implementation from the tiktoken library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "805eb2c0eea95a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:34:26.473988Z",
     "start_time": "2025-12-26T20:34:26.286655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "text1 = \"مرحبا، يوم سعيد؟\"\n",
    "text2 = \"حين طلبتُ، كلمةجديدة، من السيد هولمز\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32531b645e1a1268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:46:36.911092Z",
     "start_time": "2025-12-26T20:46:36.902170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25405, 26897, 148, 255, 39848, 12919, 148, 234, 18923, 232, 30335, 25405, 17550, 111, 44690, 22654, 38843, 148, 253, 220, 50256, 17550, 255, 22654, 23338, 17550, 115, 13862, 39848, 41486, 149, 237, 148, 234, 18923, 225, 13862, 25405, 45632, 148, 105, 38843, 22654, 38843, 45632, 148, 234, 47048, 23338, 28981, 45692, 22654, 38843, 18923, 229, 30335, 13862, 25405, 148, 110]\n",
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc0dbb7765f7cb",
   "metadata": {},
   "source": [
    "The encoding and decoding looks good.\n",
    "\n",
    "Specifically, we see that the BPE tokenizer managed to encode and decode unknown words such as كلمةجديدة correctly.\n",
    "\n",
    "This is because the algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42cf9a7153241fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T21:07:28.369819Z",
     "start_time": "2025-12-26T21:07:28.350728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25405, 26897, 148, 255, 39848, 12919]\n",
      "[149, 225, 13862, 25405, 45632, 148, 105, 38843, 22654, 38843, 45632]\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('مرحبا'))\n",
    "print(tokenizer.encode('كلمةجديدة'))\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337d3c61f52ebe8",
   "metadata": {},
   "source": [
    "But notice also the large number of token IDs compared to the previous approach based on `re.split`.\n",
    "\n",
    "To understand why, let's take a closer look at how the GPT-2 tokenization works:\n",
    "1. Convert the text to UTF-8 bytes\n",
    "2. Map bytes through a reversible “byte encoder”\n",
    "3. Iteratively apply BPE merges to combine frequent byte sequences into tokens\n",
    "\n",
    "For English text, many common sequences have merges, so you get big tokens like \" hello\" or \"ing\".\n",
    "\n",
    "For Arabic (and many non-Latin scripts), GPT-2’s merges are much weaker because the GPT-2 vocab was built from data that was heavily skewed toward English/Latin text. So Arabic often falls back to smaller byte chunks, meaning more tokens. Furthermore, the presence of diacritics such as Tashkil (تشكيل) or Harakat (حركات) for vowels, like for example in طلبتُ which includes damma/tanween, is treated as a separate Unicode point. This often breaks BPE merges which leads to more tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
