{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8071a8bcd2b819",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenizer\n",
    "\n",
    "Here we explain how the Byte Pair Encoding (BPE) Tokenizer works including a simple implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d259aedc271d10",
   "metadata": {},
   "source": [
    "## Bits and bytes\n",
    "\n",
    "Before getting into BPE details, we first need to introduce some concepts.\n",
    "\n",
    "- Since a byte consists of 8 bits, there are $2^8 = 256$ possible values that a single byte can represent, ranging from 0 to 255.\n",
    "- ASCII encoding maps these values to characters, but limited to English-only characters.\n",
    "```python\n",
    ">>> chr(97)\n",
    "'a'\n",
    ">>> ord('a')\n",
    "97\n",
    "```\n",
    "\n",
    "```text\n",
    "- A-Z -> 65-90\n",
    "- a-z -> 97-122\n",
    "- 0-9 -> 48-57\n",
    "- space -> 32\n",
    "- punctuation:\n",
    "  - . , ! ? : ; ' \" ( )\n",
    "  - + - * / =\n",
    "- control characters\n",
    "  _ \\n\n",
    "  - TAB\n",
    "```\n",
    "\n",
    "- Beyond English, we use Unicode character encoding, to map each character to a unique code point, and UTF-8 to write as bytes.\n",
    "```python\n",
    ">>>bytearray(\"ح\", \"unicode_escape\")\n",
    "bytearray(b'\\\\u062d')\n",
    "\n",
    ">>>'\\u062d'.encode('utf-8')\n",
    "b'\\xd8\\xad'\n",
    "```\n",
    "\n",
    "Note that UTF-8 is built on ASCII, which guarantees all ASCII characters map to exactly the same single byte:\n",
    "```python\n",
    ">>>bytearray(\"A\", \"ascii\")\n",
    "b'A'\n",
    ">>>bytearray(\"A\", \"utf-8\")\n",
    "b'A'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1096a9687953e00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T20:52:26.505852Z",
     "start_time": "2025-12-30T20:52:26.432122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text -> 17\n",
      "bytearray(b'This is some text') -> 17\n",
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116] -> 17\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "print(text, '->', len(text))\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary, '->',len(byte_ary))\n",
    "ids = list(byte_ary)\n",
    "print(ids, '->', len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c72ee7c1bc564",
   "metadata": {},
   "source": [
    "1. Python stores each English as a single ASCII value\n",
    "2. UTF-8 encoding turns ASCII into bytes, where each value = exactly 1 byte.\n",
    "3. Python print these directly as readable characters.\n",
    "4. When we call list() on a bytearray object, each byte is treated as an individual element, and the result is a list of integers corresponding to the byte values\n",
    "\n",
    "```css\n",
    "\"This is\"\n",
    "↓ UTF-8\n",
    "[54][68][69][73][20][69][73]\n",
    "↓ bytearray display\n",
    "b'This is'\n",
    "↓ list()\n",
    "[84, 104, 105, 115, 32, 105, 115]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76f9460bb2fedb",
   "metadata": {},
   "source": [
    "Note this would be a valid way to convert text into a token ID representation.\n",
    "\n",
    "The downside of this approach is that it creates one ID for each character: 17 character input text -> 17 token IDs.\n",
    "\n",
    "This can be even worse for Arabic text, where each character requires 2 token IDs instead of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1d7e675cc853b",
   "metadata": {},
   "source": [
    "Let's look at Arabic text more closely:\n",
    "1. Python stores each Arabic character as single Unicode scaler value\n",
    "2. UTF-8 encoding turns Arabic Unicode code points into bytes, point = exactly 2 bytes\n",
    "3. Python must print hex escapes\n",
    "\n",
    "```css\n",
    "\"مرحبا\"\n",
    "   ↓ Unicode\n",
    "[م][ر][ح][ب][ا]\n",
    "   ↓ UTF-8\n",
    "[d9 85][d8 b1][d8 ad][d8 a8][d8 a7]\n",
    "   ↓ Byte-level BPE\n",
    "[d9][85][d8][b1][d8][ad]...\n",
    "   ↓ list()\n",
    "[217, 133, 216, 177, 216, 173, 216, 168, 216, 167]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b71e2834dad876ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T20:52:36.841192Z",
     "start_time": "2025-12-30T20:52:36.794566Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا كيف حالك؟ -> 15\n",
      "bytearray(b'\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd8\\xa8\\xd8\\xa7 \\xd9\\x83\\xd9\\x8a\\xd9\\x81 \\xd8\\xad\\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\x9f') -> 28\n",
      "[217, 133, 216, 177, 216, 173, 216, 168, 216, 167, 32, 217, 131, 217, 138, 217, 129, 32, 216, 173, 216, 167, 217, 132, 217, 131, 216, 159] -> 28\n"
     ]
    }
   ],
   "source": [
    "text = \"مرحبا كيف حالك؟\"\n",
    "print(text, '->', len(text))\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary, '->',len(byte_ary))\n",
    "ids = list(byte_ary)\n",
    "print(ids, '->', len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0655b6e1888f65",
   "metadata": {},
   "source": [
    "Since Arabic uses Unicode, which requires 2 bytes in UTF-8, we see that Arabic text requires roughly twice more bytes to represent text of same length.\n",
    "\n",
    "We will see the consequences of this when looking at the BPE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027f199bf033784",
   "metadata": {},
   "source": [
    "A BPE tokenizer usually uses these 256 values as its first 256 single-character tokens; one could visually check this by running the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b6c81a2d197eef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T09:01:17.885041Z",
     "start_time": "2025-12-31T09:01:17.825037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \r\n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029d42cc2684e6b",
   "metadata": {},
   "source": [
    "## 1.2 Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98534f7340bd06f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:14:14.802939Z",
     "start_time": "2025-12-30T12:14:14.754949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text: ThisĠisĠsomeĠtext\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "\n",
    "# Preprocess: Replace spaces with 'Ġ'\n",
    "# Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "# E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "# (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "processed_text = []\n",
    "for i, char in enumerate(text):\n",
    "    if char == \" \" and i != 0:\n",
    "        processed_text.append(\"Ġ\")\n",
    "    if char != \" \":\n",
    "        processed_text.append(char)\n",
    "processed_text = \"\".join(processed_text)\n",
    "print('Processed text: {}'.format(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae322df1b60d7bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:41.990864Z",
     "start_time": "2025-12-30T12:16:41.940546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of unique chars: 256\n",
      "Extended length of unique chars: 257\n"
     ]
    }
   ],
   "source": [
    "# Initialize vocab with unique characters, including 'Ġ' if present\n",
    "# Start with the first 256 ASCII characters\n",
    "unique_chars = [chr(i) for i in range(256)]\n",
    "print(f'Initial length of unique chars: {len(unique_chars)}')\n",
    "\n",
    "# Extend unique_chars with characters from processed_text that are not already included\n",
    "unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "# Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "if 'Ġ' not in unique_chars:\n",
    "    unique_chars.append('Ġ')\n",
    "print(f'Extended length of unique chars: {len(unique_chars)}')\n",
    "\n",
    "# Now create the vocab and inverse vocab dictionaries\n",
    "vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "inverse_vocab = {char: i for i, char in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42386ea3043ab357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:42.527844Z",
     "start_time": "2025-12-30T12:16:42.517086Z"
    }
   },
   "outputs": [],
   "source": [
    "allowed_special = {\"<|endoftext|>\"}\n",
    "\n",
    "# Add allowed special tokens\n",
    "if allowed_special:\n",
    "    for token in allowed_special:\n",
    "        if token not in inverse_vocab:\n",
    "            new_id = len(vocab)\n",
    "            vocab[new_id] = token\n",
    "            inverse_vocab[token] = new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a3eedc02a62ea32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:43.287553Z",
     "start_time": "2025-12-30T12:16:43.261290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the processed_text into token IDs\n",
    "token_ids = [inverse_vocab[char] for char in processed_text]\n",
    "print(len(processed_text))\n",
    "print(len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cad401742a31806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:44.866234Z",
     "start_time": "2025-12-30T12:16:44.840829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84,\n",
       " 104,\n",
       " 105,\n",
       " 115,\n",
       " 256,\n",
       " 105,\n",
       " 115,\n",
       " 256,\n",
       " 115,\n",
       " 111,\n",
       " 109,\n",
       " 101,\n",
       " 256,\n",
       " 116,\n",
       " 101,\n",
       " 120,\n",
       " 116]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b174f2f91df3c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:48.322266Z",
     "start_time": "2025-12-30T12:16:48.294139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 115)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, deque\n",
    "\n",
    "\n",
    "def find_freq_pair(token_ids, mode=\"most\"):\n",
    "    pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "    if mode == \"most\":\n",
    "        return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "    elif mode == \"least\":\n",
    "        return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "# list(zip(token_ids, token_ids[1:]))\n",
    "#Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "pair_id = find_freq_pair(token_ids)\n",
    "pair_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b01b80de219567b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:50.039432Z",
     "start_time": "2025-12-30T12:16:50.024850Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_pair(token_ids, pair_id, new_id):\n",
    "    dq = deque(token_ids)\n",
    "    replaced = []\n",
    "\n",
    "    while dq:\n",
    "        current = dq.popleft()\n",
    "        if dq and (current, dq[0]) == pair_id:\n",
    "            print(f'Replacing {pair_id}')\n",
    "            replaced.append(new_id)\n",
    "            # Remove the 2nd token of the pair, 1st was already removed\n",
    "            dq.popleft()\n",
    "        else:\n",
    "            replaced.append(current)\n",
    "\n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18ddff4c049b1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:12:06.905912Z",
     "start_time": "2025-12-30T12:12:06.861079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq = deque(token_ids)\n",
    "current = dq.popleft()\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5265f07c10a81627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:12:07.760195Z",
     "start_time": "2025-12-30T12:12:07.739262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 104)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(current, dq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2a76d8de546a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:16:56.771543Z",
     "start_time": "2025-12-30T12:16:56.660496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing (105, 115)\n",
      "Replacing (105, 115)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[84, 104, 258, 256, 258, 256, 115, 111, 109, 101, 256, 116, 101, 120, 116]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_id = len(vocab)\n",
    "replace_pair(token_ids, pair_id, new_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cbcbc5de74af988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T12:15:35.645474Z",
     "start_time": "2025-12-30T12:15:35.572314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replace_pair(token_ids, pair_id, new_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467dc516fcb305e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "for new_id in range(len(vocab), vocab_size):\n",
    "    pair_id = find_freq_pair(token_ids, mode=\"most\")\n",
    "    if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "        break\n",
    "    token_ids = replace_pair(token_ids, pair_id, new_id)\n",
    "    # bpe_merges[pair_id] = new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f62751e23c28f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T11:42:38.864937Z",
     "start_time": "2025-12-30T11:42:38.705361Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with 'Ġ'\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Split text into tokens, keeping newlines intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # Ensure '\\n' is treated as a separate token\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # Add 'Ġ' to words that follow a space or newline\n",
    "            else:\n",
    "                tokens.append(word)  # Handle first word or standalone '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    # Uncomment for educational purposes:\n",
    "                    # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # Replace 'Ġ' with a space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f76f2bf801bd21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
