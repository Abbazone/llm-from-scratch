{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Working with text data\n",
    "\n",
    "Here we will see how to prepare input text for training LLMs. This involves splitting the text into individual word and subword tokens, which can be encoded into vector representations for the LLM."
   ],
   "id": "914f14b64115aded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1 Understanding word embeddings\n",
    "\n",
    "Deep neural network models, including LLMs, cannot process raw text directly. Therefore, we need a way to represent words as continous-valued vectors.\n",
    "\n",
    "The concept of converting data into a vector format is often referred to as embedding.\n",
    "\n",
    "Word embeddings can have varying dimensions, from one to thousands."
   ],
   "id": "c99f31a3d1879071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Tokenizing text\n",
    "\n",
    "Let's see how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM.\n",
    "\n",
    "We start with a simple text and Python’s `re.split` function to split the text while keeping the delimiters:"
   ],
   "id": "9650dbd08cb40ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:15.545662Z",
     "start_time": "2025-12-26T20:32:15.534024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "RE_ENCODER = r'([،.؟!:؛«»—]|\\s)'\n",
    "RE_DECODER = r'\\s+([،.؟!:؛—])'\n",
    "\n",
    "text = \"مرحبا، بالعالم. هل هذا— اختبار؟\"\n",
    "result = re.split(RE_ENCODER, text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n",
    "print(result[0])\n",
    "print(result[-1])"
   ],
   "id": "365e647f4bd7231b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مرحبا', '،', 'بالعالم', '.', 'هل', 'هذا', '—', 'اختبار', '؟']\n",
      "مرحبا\n",
      "؟\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice that “right-to-left” display of Arabic text is a rendering/visual property, not how the string is stored or indexed.\n",
    "\n",
    "In Python, a `str` is a sequence of Unicode code points stored in the order it was typed in (the logical order), so `result[0] = مرحبا` and `result[-1] = ؟` regardless of whether it's displayed right-to-left or left-to-right."
   ],
   "id": "60ab64420a72dca5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The text we will tokenize for LLM training is \"مغامرة العميل المرموق\", which has been released into the public domain and is thus permitted to be used for LLM training tasks.\n",
    "The text is available on Researchdata.se as part of [The Arabic E-Book Corpus](https://doi.org/10.5878/7rbh-gy93)."
   ],
   "id": "2a1abb6603663640"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:18.145949Z",
     "start_time": "2025-12-26T20:32:18.087151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"مغامرة-العميل-المرموق.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/Abbazone/\"\n",
    "        \"llm-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"العميل.txt\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ],
   "id": "6c0bd4fb3c2d8976",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:18.699350Z",
     "start_time": "2025-12-26T20:32:18.694176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(f'Total number of characters: {len(raw_text)}')\n",
    "print(raw_text[:500])"
   ],
   "id": "440af4e8f80f5129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 44740\n",
      "مغامرة العميل المرموق\n",
      "\n",
      "تأليف\n",
      "آرثر كونان دويل\n",
      "\n",
      "ترجمة\n",
      "دينا عادل غراب\n",
      "\n",
      "مراجعة\n",
      "شيماء طه الريدي\n",
      "\n",
      "مغامرة العميل المرموق\n",
      "\n",
      "حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.» لأحصل بذلك أخيرًا على الإذن بتدوينِ ما كان — من بضع نواحٍ — اللحظةَ الأبرز والأهم في مسيرة صديقي المهنية ذات يوم.\n",
      "\n",
      "كان لدينا، أنا وهولمز ضَعْفٌ تجاهَ الحَمَّام التركي؛ فلم أجده أقلَّ تحفظًا وأكثر آدميةً كما كان وسطَ البخار في أجواء التراخي الممتعة في حجرة التجفيف\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's apply our basic tokenizer to the main text:",
   "id": "dfb786ef419201f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:19.841010Z",
     "start_time": "2025-12-26T20:32:19.833135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(RE_ENCODER, raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f'Number of tokens in the text: {len(preprocessed)}')\n",
    "print(f'Number of unique tokens in the text: {len(set(preprocessed))}')\n",
    "print(f'First 30 tokens in the text:\\n{preprocessed[:100]}')"
   ],
   "id": "5effd77561e01657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the text: 9399\n",
      "Number of unique tokens in the text: 3890\n",
      "First 30 tokens in the text:\n",
      "['مغامرة', 'العميل', 'المرموق', 'تأليف', 'آرثر', 'كونان', 'دويل', 'ترجمة', 'دينا', 'عادل', 'غراب', 'مراجعة', 'شيماء', 'طه', 'الريدي', 'مغامرة', 'العميل', 'المرموق', 'حين', 'طلبتُ', 'الإذنَ', 'من', 'السيد', 'هولمز', '،', 'للمرة', 'العاشرة', 'خلال', 'عدة', 'سنوات', '،', 'للبوح', 'بالقصة', 'التالية', '،', 'أجابني', 'بقوله', ':', '«', 'لا', 'ضررَ', 'من', 'ذلك', 'الآن', '.', '»', 'لأحصل', 'بذلك', 'أخيرًا', 'على', 'الإذن', 'بتدوينِ', 'ما', 'كان', '—', 'من', 'بضع', 'نواحٍ', '—', 'اللحظةَ', 'الأبرز', 'والأهم', 'في', 'مسيرة', 'صديقي', 'المهنية', 'ذات', 'يوم', '.', 'كان', 'لدينا', '،', 'أنا', 'وهولمز', 'ضَعْفٌ', 'تجاهَ', 'الحَمَّام', 'التركي', '؛', 'فلم', 'أجده', 'أقلَّ', 'تحفظًا', 'وأكثر', 'آدميةً', 'كما', 'كان', 'وسطَ', 'البخار', 'في', 'أجواء', 'التراخي', 'الممتعة', 'في', 'حجرة', 'التجفيف', '.', 'يوجد', 'في', 'الدور']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3 Converting tokens into IDs\n",
    "\n",
    "Next let's convert these tokens from a Python string to an integer representation to produce the token IDs.\n",
    "\n",
    "To do this we need to build a vocabulary. This defines how we map each unique token to a unique integer."
   ],
   "id": "36d511c7a8af661e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:20.892777Z",
     "start_time": "2025-12-26T20:32:20.887758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ],
   "id": "e0139b150ccdecd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3890\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:21.405892Z",
     "start_time": "2025-12-26T20:32:21.399604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "str_to_int = {token: i for i, token in enumerate(all_words)}    #\n",
    "for i, item in enumerate(str_to_int.items()):\n",
    "    print(f'{i}: {item}')\n",
    "    if i >= 50:\n",
    "        break"
   ],
   "id": "8d46d483b5662efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('!', 0)\n",
      "1: ('.', 1)\n",
      "2: (':', 2)\n",
      "3: ('«', 3)\n",
      "4: ('»', 4)\n",
      "5: ('،', 5)\n",
      "6: ('؛', 6)\n",
      "7: ('؟', 7)\n",
      "8: ('آبَهُ', 8)\n",
      "9: ('آتٍ', 9)\n",
      "10: ('آثار', 10)\n",
      "11: ('آجلًا', 11)\n",
      "12: ('آخر', 12)\n",
      "13: ('آخرون', 13)\n",
      "14: ('آخرين', 14)\n",
      "15: ('آخِر', 15)\n",
      "16: ('آدميةً', 16)\n",
      "17: ('آرثر', 17)\n",
      "18: ('آلت', 18)\n",
      "19: ('آن', 19)\n",
      "20: ('آنسة', 20)\n",
      "21: ('آه', 21)\n",
      "22: ('أبديت', 22)\n",
      "23: ('أبراج', 23)\n",
      "24: ('أبرهن', 24)\n",
      "25: ('أبوها', 25)\n",
      "26: ('أبي', 26)\n",
      "27: ('أبيها', 27)\n",
      "28: ('أتابع', 28)\n",
      "29: ('أتابِعَ', 29)\n",
      "30: ('أتاح', 30)\n",
      "31: ('أتحدث', 31)\n",
      "32: ('أتحرى', 32)\n",
      "33: ('أتخيل', 33)\n",
      "34: ('أتسألني', 34)\n",
      "35: ('أتعابَك', 35)\n",
      "36: ('أتقصد', 36)\n",
      "37: ('أتوقَّع', 37)\n",
      "38: ('أتيت', 38)\n",
      "39: ('أتينا', 39)\n",
      "40: ('أثارت', 40)\n",
      "41: ('أثر', 41)\n",
      "42: ('أثره', 42)\n",
      "43: ('أثرٍ', 43)\n",
      "44: ('أثرِ', 44)\n",
      "45: ('أجابني', 45)\n",
      "46: ('أجبته', 46)\n",
      "47: ('أجد', 47)\n",
      "48: ('أجده', 48)\n",
      "49: ('أجدها', 49)\n",
      "50: ('أجرة', 50)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need also a way to turn token IDs into text. For this we create an inverse version of the vocabulary that maps token IDs back to text tokens:",
   "id": "17232db75593100d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:22.608600Z",
     "start_time": "2025-12-26T20:32:22.605782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "int_to_str = {i: s for s, i in str_to_int.items()}\n",
    "print(int_to_str[50])"
   ],
   "id": "dd42f793c1fe0841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أجرة\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We are now ready to implement a complete tokenizer class in Python with the following features:\n",
    "- an `encode` method that splits text into tokens and carries out the string-to-integer mapping.\n",
    "- a `decode` method that carries out the reverse integer-to-string mapping to convert token IDs back to text."
   ],
   "id": "b65ddd711c0bd643"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:24.940673Z",
     "start_time": "2025-12-26T20:32:24.936529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(RE_ENCODER, text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(RE_DECODER, r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        text = re.sub(r\"«\\s+\", \"«\", text)\n",
    "        text = re.sub(r\"\\s+»\", \"»\", text)\n",
    "        return text\n"
   ],
   "id": "6e6cb6865d172ea3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|We can now use the tokenizer to encode texts into integers:",
   "id": "3d0bfceed9a3c7dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:26.455024Z",
     "start_time": "2025-12-26T20:32:26.451025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab=str_to_int)\n",
    "text = \"\"\"حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.»\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "id": "6098b085533450ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1898, 2382, 524, 3191, 787, 3351, 5, 2919, 869, 1943, 2423, 2187, 5, 2900, 1318, 579, 5, 45, 1446, 2, 3, 2803, 2349, 3191, 2014, 458, 1, 4]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now try to turn these token IDs back into text using the `decode` method:",
   "id": "4edb73649bbe218"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:27.697062Z",
     "start_time": "2025-12-26T20:32:27.693379Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(ids))",
   "id": "8980fcdd5e1fc685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حين طلبتُ الإذنَ من السيد هولمز، للمرة العاشرة خلال عدة سنوات، للبوح بالقصة التالية، أجابني بقوله: «لا ضررَ من ذلك الآن.»\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on this output we can see that the decode method worked successfully.\n",
    "\n",
    "Let's now apply our tokenizer to text sample not contained in the training set:"
   ],
   "id": "2503443c145edd20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:29.198295Z",
     "start_time": "2025-12-26T20:32:29.152880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"مرحبا، يوم سعيد؟\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "a71ed8b0133f4490",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'مرحبا'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m text = \u001B[33m\"\u001B[39m\u001B[33mمرحبا، يوم سعيد؟\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(tokenizer.decode(tokenizer.encode(text)))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mSimpleTokenizerV1.encode\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m      7\u001B[39m preprocessed = re.split(RE_ENCODER, text)\n\u001B[32m      8\u001B[39m preprocessed = [item.strip() \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m preprocessed \u001B[38;5;28;01mif\u001B[39;00m item.strip()]\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m ids = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstr_to_int\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m preprocessed]\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m ids\n",
      "\u001B[31mKeyError\u001B[39m: 'مرحبا'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We got `str_to_int['مرحبا']` key error. The problem is that the word \"مرحبا\" was not used in the original text. Hence, it is not contained in the vocabulary.",
   "id": "66d6858e10d6dc95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4 Adding special context tokens\n",
    "\n",
    "We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a models understanding of context or other relevant information of the text.\n",
    "\n",
    "We will modify the vocabulary and tokenizer to support two new tokens `<|unk|>` and `<|endoftext|>`.\n",
    "\n",
    "Let's start by extending our vocabularly with the new tokens `<|unk|>` and `<|endoftext|>`:"
   ],
   "id": "1c2f84cbdf37bcb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:33.712461Z",
     "start_time": "2025-12-26T20:32:33.704863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "print(f'Original vocabulary size: {len(str_to_int)}')\n",
    "all_tokens.extend(['<|unk|>', '<|endoftext|>'])\n",
    "str_to_int = {s: i for i, s in enumerate(all_tokens)}\n",
    "print(f'Extended vocabulary size: {len(str_to_int)}')"
   ],
   "id": "3808c71e54f966ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 3890\n",
      "Extended vocabulary size: 3892\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:38.007681Z",
     "start_time": "2025-12-26T20:32:37.991910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s, i in list(str_to_int.items())[-5:]:\n",
    "    print(f'{s}: {i}')"
   ],
   "id": "55ddbd9033f204fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—: 3887\n",
      "…: 3888\n",
      "ﻟ: 3889\n",
      "<|unk|>: 3890\n",
      "<|endoftext|>: 3891\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|We confirm that the two new special tokens were successfully incorporated into the vocabulary.",
   "id": "f86d7ec5f3a7e65b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We are now ready to implement a complete tokenizer class in Python with the following features:\n",
    "- an `encode` method that splits text into tokens and carries out the string-to-integer mapping.\n",
    "- unknown words that are not part of the vocabulary must be mapped to special token `<|unk|>`.\n",
    "- independent text sources must be separated by special token `<|endoftext|>`.\n",
    "- a `decode` method that carries out the reverse integer-to-string mapping to convert token IDs back to text.\n"
   ],
   "id": "a712c8bca3c90b56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:44.969904Z",
     "start_time": "2025-12-26T20:32:44.963809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(RE_ENCODER, text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item.strip() if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub(RE_DECODER, r'\\1', text)    # remove extra spaces before punctuation.\n",
    "        text = re.sub(r\"«\\s+\", \"«\", text)\n",
    "        text = re.sub(r\"\\s+»\", \"»\", text)\n",
    "        return text\n"
   ],
   "id": "96f520683694ad5a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now apply the tokenizer to a new text sample not contained in the training set:",
   "id": "542c4596ea69961a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:32:47.344736Z",
     "start_time": "2025-12-26T20:32:47.338504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"مرحبا، يوم سعيد؟\"\n",
    "tokenizer = SimpleTokenizerV2(vocab=str_to_int)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "4c76c2e8919e3efe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3890, 5, 3852, 2172, 7]\n",
      "<|unk|>، يوم سعيد؟\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first word مرحبا has successfully been mapped to token ID 3890 and back to `<|unk|>`.\n",
    "\n",
    "Let's now try to combine two independent texts:"
   ],
   "id": "2b00ab19e9a1e7a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:44:45.440689Z",
     "start_time": "2025-12-26T11:44:45.430996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text1 = \"مرحبا، يوم سعيد؟\"\n",
    "text2 = \"حين طلبتُ، كلمةجديدة، من السيد هولمز\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ],
   "id": "8d8860ed2c3d2464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:41:20.514773Z",
     "start_time": "2025-12-26T11:41:20.509894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "42d90c234103c80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3890, 5, 3852, 2172, 7, 3891, 1898, 2382, 5, 3890, 5, 3191, 787, 3351]\n",
      "<|unk|>، يوم سعيد؟ <|endoftext|> حين طلبتُ، <|unk|>، من السيد هولمز\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.5 Byte pair encoding\n",
    "\n",
    "The Byte Pair Encoder (BPE) was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "\n",
    "Let's first look at an existing implementation from the tiktoken library:"
   ],
   "id": "e2351c4a266aec91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:34:26.473988Z",
     "start_time": "2025-12-26T20:34:26.286655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "text1 = \"مرحبا، يوم سعيد؟\"\n",
    "text2 = \"حين طلبتُ، كلمةجديدة، من السيد هولمز\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ],
   "id": "805eb2c0eea95a70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T20:46:36.911092Z",
     "start_time": "2025-12-26T20:46:36.902170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ],
   "id": "32531b645e1a1268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25405, 26897, 148, 255, 39848, 12919, 148, 234, 18923, 232, 30335, 25405, 17550, 111, 44690, 22654, 38843, 148, 253, 220, 50256, 17550, 255, 22654, 23338, 17550, 115, 13862, 39848, 41486, 149, 237, 148, 234, 18923, 225, 13862, 25405, 45632, 148, 105, 38843, 22654, 38843, 45632, 148, 234, 47048, 23338, 28981, 45692, 22654, 38843, 18923, 229, 30335, 13862, 25405, 148, 110]\n",
      "مرحبا، يوم سعيد؟ <|endoftext|> حين طلبتُ، كلمةجديدة، من السيد هولمز\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The encoding and decoding looks good.\n",
    "\n",
    "Specifically, we see that the BPE tokenizer managed to encode and decode unknown words such as كلمةجديدة correctly.\n",
    "\n",
    "This is because the algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words."
   ],
   "id": "d5fc0dbb7765f7cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T21:07:28.369819Z",
     "start_time": "2025-12-26T21:07:28.350728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.encode('مرحبا'))\n",
    "print(tokenizer.encode('كلمةجديدة'))\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ],
   "id": "42cf9a7153241fbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25405, 26897, 148, 255, 39848, 12919]\n",
      "[149, 225, 13862, 25405, 45632, 148, 105, 38843, 22654, 38843, 45632]\n",
      "[50256]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But notice also the large number of token IDs compared to the previous approach based on `re.split`.\n",
    "\n",
    "To understand why, let's take a closer look at how the GPT-2 tokenization works:\n",
    "1. Convert the text to UTF-8 bytes\n",
    "2. Map bytes through a reversible “byte encoder”\n",
    "3. Iteratively apply BPE merges to combine frequent byte sequences into tokens\n",
    "\n",
    "For English text, many common sequences have merges, so you get big tokens like \" hello\" or \"ing\".\n",
    "\n",
    "For Arabic (and many non-Latin scripts), GPT-2’s merges are much weaker because the GPT-2 vocab was built from data that was heavily skewed toward English/Latin text. So Arabic often falls back to smaller byte chunks, meaning more tokens. Furthermore, the presence of diacritics such as Tashkil (تشكيل) or Harakat (حركات) for vowels, like for example in طلبتُ which includes damma/tanween, is treated as a separate Unicode point. This often breaks BPE merges which leads to more tokens."
   ],
   "id": "1337d3c61f52ebe8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
